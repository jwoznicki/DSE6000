{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Start a new session\n",
    "session = requests.Session()\n",
    "\n",
    "\n",
    "# Pull the data from the Kaggle Kernels:\n",
    "# iterate through each kaggle kernel in the json\n",
    "\n",
    "# limit how many kernels pages we pull\n",
    "stop_limit = 20000\n",
    "\n",
    "# initialize the iterator to 1\n",
    "i = 0\n",
    "\n",
    "# initialize the dataframes\n",
    "KernelDim = pd.DataFrame()\n",
    "Bad_KernelDim = pd.DataFrame()\n",
    "# Start the loop\n",
    "while i < stop_limit:\n",
    "    try:\n",
    "        # Load the new json file\n",
    "        kernel_url = \"https://www.kaggle.com/kernels.json?sortBy=votes&pageSize=1&startRow=\" + str(i)\n",
    "        connect = session.get(kernel_url, verify=False)\n",
    "        json_file = connect.json()\n",
    "\n",
    "        # Pull all data for the kernel\n",
    "        kernel_temp = pd.DataFrame.from_records(json_file)\n",
    "\n",
    "        # if the kernel json is empty or if we hit 10000 records then stop\n",
    "        if kernel_temp.empty:\n",
    "            break\n",
    "\n",
    "        # Append the new data\n",
    "        KernelDim = KernelDim.append(kernel_temp)\n",
    "\n",
    "    # if there is an exception, log it\n",
    "    except Exception:\n",
    "        Bad_KernelDim = Bad_KernelDim.append(pd.DataFrame(data={'url':kernel_url, 'error':'exception'}, index=[i]))\n",
    "        pass\n",
    "    i+=1\n",
    "\n",
    "# Change the index to the KernelID\n",
    "KernelDim = KernelDim.reset_index()\n",
    "KernelDim.to_json(\"KernelDim.json\")\n",
    "\n",
    "\n",
    "## Pull the user rankings for Kernels (EDA)\n",
    "#limit how many kernel rankings we pull - good idea is stop_limit_rank/20\n",
    "stop_limit_ranks = max(int(stop_limit/20),1)\n",
    "\n",
    "j = 1\n",
    "\n",
    "# initialize the dataframes\n",
    "Ranks = pd.DataFrame()\n",
    "Bad_Ranks = pd.DataFrame()\n",
    "while j <= stop_limit_ranks:\n",
    "    try:\n",
    "        # Load the new json file\n",
    "        ranks_url = \"https://www.kaggle.com/rankings.json?group=kernels&page=\" + str(j)\n",
    "        connect = session.get(ranks_url, verify=False)\n",
    "        json_file = connect.json()\n",
    "        \n",
    "        # Pull all data for the kernel\n",
    "        ranks_temp = pd.DataFrame.from_records(json_normalize(json_file).list[0])\n",
    "\n",
    "        # If the ranks json is empty, quit\n",
    "        if ranks_temp.empty:\n",
    "            break\n",
    "\n",
    "        # Append the new data\n",
    "        Ranks = Ranks.append(ranks_temp)\n",
    "       \n",
    "    # If an exception is thrown, log it\n",
    "    except Exception:\n",
    "        Bad_Ranks = Bad_Ranks.append(pd.DataFrame(data={'url':ranks_url, 'error':'Exception'}, index=[j]))\n",
    "        pass\n",
    "    j+=1\n",
    "\n",
    "# Change the index from joining\n",
    "Ranks.set_index('userId', inplace=True)\n",
    "\n",
    "## Pull the country for each of the ranked users\n",
    "# Initialize the dataframes\n",
    "UserCountry = pd.DataFrame()\n",
    "Bad_UserCountry = pd.DataFrame()\n",
    "# loop through each user URL\n",
    "for index, row in Ranks.iterrows():\n",
    "    try:\n",
    "        # Load the html from the url\n",
    "        country_url = \"https://www.kaggle.com\"+str(row.userUrl)\n",
    "        country_page = session.get(country_url, verify=False)\n",
    "        country_html = country_page.text\n",
    "        country_soup = BeautifulSoup(country_html, \"lxml\")\n",
    "\n",
    "        # Find the main content on the page\n",
    "        country_json = country_soup.find('div',{'class':'site-layout__main-content'}).find('script').text\n",
    "           \n",
    "        # Only get the relevant html from the page\n",
    "        sLoc = country_json.find(\"Kaggle.State.push(\")+len(\"Kaggle.State.push(\")\n",
    "        eLoc = country_json.find(\");performance && performance.mark\")\n",
    "        if sLoc == -1:\n",
    "            sLoc = 0\n",
    "        if eLoc == -1:\n",
    "            eLoc = len(country_json)\n",
    "                      \n",
    "        # Trim the fat!\n",
    "        country_json = country_json[sLoc:eLoc]\n",
    "                     \n",
    "        # Read the remaining json into a dataframe\n",
    "        df = pd.read_json(country_json)\n",
    "\n",
    "        # We only want the data from the first row. There's a lot of junk...\n",
    "        d = {'userId':df.iloc[0].userId, 'country':df.iloc[0].country}\n",
    "        \n",
    "        # Add the data: UserID, Country\n",
    "        UserCountry = UserCountry.append(pd.DataFrame(data=d, index=[0]))\n",
    "            \n",
    "    # Log exceptions\n",
    "    except Exception:\n",
    "        Bad_UserCountry = Bad_UserCountry.append(pd.DataFrame(data={'url':country_url, 'error':'Exception'}, index=[0]))\n",
    "        break\n",
    "\n",
    "# Change the index for joining\n",
    "UserCountry.set_index('userId', inplace=True)\n",
    "# Join\n",
    "Ranks = Ranks.join(UserCountry)\n",
    "Ranks.to_json(\"Ranks.json\")\n",
    "\n",
    "# Create a function to flatten lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "k = 0\n",
    "                           \n",
    "# Initialize the packages dataframe\n",
    "Packages = pd.DataFrame()\n",
    "Code_Lines = pd.DataFrame()\n",
    "Bad_Packages = pd.DataFrame()\n",
    "# Only look at python kernels\n",
    "Python_Kernels = KernelDim[KernelDim.languageName == 'Python']\n",
    "# Loop through each of the kernels\n",
    "for index, row in Python_Kernels.iterrows():\n",
    "    try:\n",
    "        # Load the HTML from the URL\n",
    "        script_url = \"https://www.kaggle.com\"+row.scriptUrl\n",
    "        script_page = session.get(script_url, verify=False)\n",
    "        script_html = script_page.text\n",
    "        script_soup = BeautifulSoup(script_html, \"lxml\")\n",
    "\n",
    "        # Only pull html from the main content\n",
    "        script_json = script_soup.find('div',{'class':'site-layout__main-content'}).find('script').text\n",
    "\n",
    "        # Get rid of the irrelevant code\n",
    "        sLoc = script_json.find(\"Kaggle.State.push(\")+len(\"Kaggle.State.push(\")\n",
    "        eLoc = script_json.find(\");performance && performance.mark\")\n",
    "        if sLoc == -1:\n",
    "            sLoc = 0\n",
    "        if eLoc == -1:\n",
    "            eLoc = len(script_json)\n",
    "        # Trim the fat!           \n",
    "        script_json = script_json[sLoc:eLoc]\n",
    "\n",
    "        cells = pd.DataFrame.from_records(json_normalize(json.loads(json_normalize(json.loads(script_json)).code[0])).cells[0])\n",
    "        source_code = cells[cells.cell_type == 'code'].reset_index()['source']\n",
    "        df = source_code.to_frame()\n",
    "        df['kernel_id'] = row.id\n",
    "        Code_Lines = Code_Lines.append(df)\n",
    "\n",
    "        packages_in_snippet = list()\n",
    "        for snippet in source_code:\n",
    "            packages_in_line = list()\n",
    "            for line in snippet:\n",
    "\n",
    "                line = line.replace(\"\\n\",\"\")\n",
    "                # Find comments and remove everything after them\n",
    "                comment_ind = line.find(\"#\")\n",
    "                if comment_ind == -1:\n",
    "                    comment_ind = len(line)\n",
    "                line = line[0:comment_ind]\n",
    "\n",
    "                # if there isn't an import statement, skip it\n",
    "                if line.find(\"import \") == -1:\n",
    "                    continue\n",
    "                # if the line starts with import, then split on commas and remove the import part\n",
    "                elif line.find(\"import \") == 0:\n",
    "                    line_split = line.split(\",\")\n",
    "                    line_split[0] = line_split[0].replace(\"import \",\"\").strip()\n",
    "                # if the line starts with from, split it on import, remove the from, then split on commas\n",
    "                elif line.find(\"from\") == 0:\n",
    "                    line_split = line.split(\" import \")\n",
    "                    line_split[0] = line_split[0].replace(\"from \",\"\")\n",
    "                    line_split[1] = line_split[1].split(\", \")\n",
    "                    # add in the main package (in format package.subpackage)\n",
    "                    for i in range(len(line_split[1])):\n",
    "                        line_split[1][i] = line_split[0]+'.'+line_split[1][i].strip()\n",
    "                    line_split = line_split[1]\n",
    "\n",
    "                # Add the packages to the list\n",
    "                packages_in_line.append(line_split)\n",
    "            packages_in_snippet.append(packages_in_line)\n",
    "\n",
    "        packages_flat = flatten(flatten(packages_in_snippet))\n",
    "\n",
    "        # separate out the aliases\n",
    "        for item in packages_flat:\n",
    "            item_split = item.split(\" as \")\n",
    "            # Pull the last item if the import uses \".\" as in import pandas.io.json\n",
    "            last_package = item_split[0].split(\".\")[-1]\n",
    "            # If there is no alias, reuse the package\n",
    "            if len(item_split) == 1:\n",
    "                d = {'package call':item_split[0], 'package':last_package, 'alias':last_package, 'kernel_id': row.id}\n",
    "            else:\n",
    "                d = {'package call':item_split[0], 'package':last_package, 'alias':item_split[1], 'kernel_id': row.id}\n",
    "\n",
    "            Packages = Packages.append(pd.DataFrame(data=d, index=[k]))\n",
    "            k += 1\n",
    "                    \n",
    "    except Exception:\n",
    "        Bad_Packages = Bad_Packages.append(pd.DataFrame(data={'url':row.scriptUrl, 'error':'Exception'}, index=[0]))\n",
    "        pass\n",
    "    \n",
    "Packages.to_json(\"Packages.json\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import re\n",
    "\n",
    "    Method_Calls = pd.DataFrame()\n",
    "    for index, row in Python_Kernels.iterrows():\n",
    "        Kernel_Packages = Packages[Packages.kernel_id == row.id]\n",
    "        kernel_source_code = Code_Lines[Code_Lines.kernel_id == row.id]['source']\n",
    "\n",
    "\n",
    "        lines_with_funcs = pd.DataFrame()\n",
    "        for snippet in kernel_source_code:\n",
    "            for line in snippet:\n",
    "\n",
    "                line = line.replace(\"\\n\",\"\")\n",
    "                # Find comments and remove everything after them\n",
    "                comment_ind = line.find(\"#\")\n",
    "                if comment_ind == -1:\n",
    "                    comment_ind = len(line)\n",
    "                line = line[0:comment_ind]\n",
    "\n",
    "                # if there IS an import statement, skip it\n",
    "                if line.find(\"import \") >= 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    for index2, rows2 in Kernel_Packages.iterrows():\n",
    "                        if line.find(str(rows2.alias)+\".\") >= 0:\n",
    "                            d = {'call':str(rows2.alias), 'line':line, 'index_filter': rows2.kernel_id}\n",
    "                            lines_with_funcs = lines_with_funcs.append(pd.DataFrame(data=d, index=[rows2.kernel_id]))\n",
    "                        elif line.find(str(rows2.package)+\".\") >= 0:\n",
    "                            d = {'call':str(rows2.package), 'line':line, 'index_filter': rows2.kernel_id}\n",
    "                            lines_with_funcs = lines_with_funcs.append(pd.DataFrame(data=d, index=[rows2.kernel_id]))\n",
    "                        elif line.find(str(rows2[['package call']])+\".\") >= 0:\n",
    "                            d = {'call':str(rows2[['package call']]), 'line':line, 'index_filter': rows2.kernel_id}\n",
    "                            lines_with_funcs = lines_with_funcs.append(pd.DataFrame(data=d, index=[rows2.kernel_id]))\n",
    "\n",
    "        for index3, rows3 in lines_with_funcs.iterrows():\n",
    "            line_list = re.findall(rows3.call+'\\.\\w+\\(', rows3.line)\n",
    "            for line in line_list:\n",
    "                sLoc = len(str(rows3.call))+1\n",
    "                eLoc = len(line)-1\n",
    "                df = Kernel_Packages[Kernel_Packages.kernel_id == rows3.index_filter]\n",
    "                df['method'] = line[sLoc:eLoc]\n",
    "                Method_Calls = Method_Calls.append(df)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "Method_Calls = Method_Calls.reset_index()\n",
    "Method_Calls.to_json(\"Method_Calls.json\")\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print(\"Runtime: \"+str((end_time-start_time).seconds/3600)+\" hours\")\n",
    "print(\" \")\n",
    "print(\"Kernels Parsed: \"+str(len(KernelDim.index)))\n",
    "print(\"Users Ranked: \"+str(len(Ranks.index)))\n",
    "print(\"Bad records: \"+str(len(Bad_Ranks.index)+len(Bad_UserCountry.index)+len(Bad_KernelDim.index)+len(Bad_Packages.index)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
